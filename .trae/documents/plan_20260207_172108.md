# Implement Data Cleaning & Preprocessing with System DB Storage

I will implement the data cleaning features and ensure the processed data is saved to the system's database and automatically registered for management.

## 1. Frontend: Task Configuration UI (`TasksPage.jsx`)
I will update the **Task Creation Modal** to include a "Data Processing" workflow:

*   **Processing Options**:
    *   **Exploratory Analysis**: Generates a data profile (stats/nulls).
    *   **Cleaning**: Handle Missing Values (Fill/Drop), Deduplication, Outliers (IQR).
    *   **Transformation**: Standardization (Z-Score), Column Renaming.
*   **Target Configuration**:
    *   Allow selecting **System MySQL** or **System ClickHouse** as the destination for cleaned data.
    *   Input for the **Target Table Name**.

## 2. Backend: Processing Engine & Storage
I will update the backend to execute these operations and manage the data lifecycle.

### A. Operators (`backend/operators/`)
*   Implement `exploration.py`, `outliers.py`, `transformation.py`, and update `missing.py`.
*   Ensure all operators support both **PySpark** (for large scale) and **Pandas** (fallback).

### B. Job Script (`preprocess_job.py`)
I will enhance the job script to:
1.  **Support ClickHouse & MySQL**: Implement robust read/write logic for both system databases using `clickhouse-driver` and `sqlalchemy`.
2.  **Auto-Registration**: **Crucial Step** - Upon successful completion, the script will automatically register the new table in the `SyncedTable` database. This ensures the cleaned data immediately appears in the **Data Management** page.
3.  **Execute Operators**: Apply the selected cleaning/transformation steps in sequence.

## 3. Execution Flow
1.  User configures task with cleaning options and target table name (e.g., `cleaned_data_v1`).
2.  System runs the job (Spark or Pandas).
3.  Job reads source -> cleans data -> writes to System DB -> **Registers `cleaned_data_v1` in Asset Manager**.
4.  User sees `cleaned_data_v1` in the Data Management page.
