# 数据预处理系统后端开发计划

根据 PRD 文档，本项目将构建一个基于 Python 的后端系统，通过 API 管理数据源与任务，并调用 Spark 进行实际的数据处理。

## 1. 项目初始化与架构搭建
- **技术栈**: 
  - Web 框架: FastAPI (高性能，易于构建 REST API)
  - ORM: SQLModel / SQLAlchemy (元数据管理)
  - 数据处理: PySpark (对接 Spark 引擎)
  - 数据库: SQLite (开发阶段元数据存储，可切换 MySQL)
- **目录结构**:
  - `backend/app`: API 服务核心代码 (API, Models, Services)
  - `backend/spark_jobs`: PySpark 任务脚本 (执行层)
  - `backend/operators`: 可复用的 Spark 算子逻辑
  - `data/`: 本地模拟数据存储目录

## 2. 核心功能模块开发

### 2.1 元数据管理模块 (API Layer)
- **数据源管理**: 实现数据源 (MySQL, ClickHouse, MinIO 等) 的增删改查 API，以及连接测试功能。
- **任务管理**: 定义数据同步与预处理任务的配置模型，保存任务参数（如源表、目标表、处理算子配置）。
- **审计日志**: 记录用户操作行为。

### 2.2 Spark 任务引擎 (Engine Layer)
- **通用任务脚本**:
  - `full_sync_job.py`: 全量数据同步任务。
  - `preprocess_job.py`: 动态预处理任务，支持根据 JSON 配置加载算子。
- **算子库 (`operators/`)**:
  - 实现去重、缺失值填充、异常值处理、类型转换等基础算子。
  - 实现时序数据处理算子（时间对齐、重采样）。

### 2.3 任务调度与执行
- **任务提交**: 在 API 服务中通过 `subprocess` 调用 `spark-submit` 提交任务。
- **状态追踪**: 任务启动后记录 Application ID，并提供接口查询任务状态（开发阶段可通过轮询或日志监控）。

## 3. 开发步骤详情

### 第一阶段：基础框架与数据源管理
1. 创建项目结构与 `requirements.txt`。
2. 配置 FastAPI 与数据库连接。
3. 实现 `DataSource` 的 CRUD 接口与连接测试逻辑。

### 第二阶段：预处理任务配置与 Spark 基础作业
1. 定义预处理任务的 JSON 配置结构。
2. 开发 `preprocess_job.py`，实现解析 JSON 配置并构建 Spark DataFrame 处理链的逻辑。
3. 实现基础算子：去重 (`dedup`)、缺失值填充 (`missing_fill`)。

### 第三阶段：任务提交与集成测试
1. 开发 API 接口用于触发 Spark 任务。
2. 编写测试脚本，模拟从 API 创建任务到 Spark 执行的全流程。
3. 验证数据回写功能。

## 4. 验证计划
- 使用本地 CSV/Parquet 文件模拟数据源和目标存储。
- 通过 API 创建一个“去重+填充缺失值”的任务。
- 检查 Spark 执行日志与输出结果文件。
