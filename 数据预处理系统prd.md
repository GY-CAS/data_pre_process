# 数据预处理系统 PRD（产品需求文档）

## 1. 项目背景与目标

### 1.1 背景
随着多业务系统、多数据平台并行运行，数据分散在不同数据库、对象存储和文件系统中，数据格式不统一、质量参差不齐，直接影响后续分析、建模和智能应用效果。因此需要建设一个**统一的数据预处理系统**，实现跨平台数据引接、集中管理和标准化预处理。

### 1.2 建设目标
- 支持从主流数据平台和存储系统按需引接数据
- 提供可视化的数据管理与预处理能力，降低技术使用门槛
- 基于 Flink / Spark 实现大规模数据的批流处理
- 对用户操作进行全流程审计和异常预警

### 1.3 适用用户
- 数据工程师
- 算法工程师
- 数据分析人员
- 平台运维与安全审计人员

---

## 2. 系统总体架构

### 2.1 架构概览

- **前端层**：
  - Web 可视化界面（数据源配置、任务管理、数据管理、预处理配置）

- **服务层**：
  - 元数据管理服务
  - 数据源管理服务
  - 任务调度与状态管理服务
  - 用户行为审计与告警服务

- **计算层**：
  - Flink（流式同步、实时预处理）
  - Spark（批量数据同步与复杂预处理）

- **存储层**：
  - 业务数据库（MySQL / ClickHouse 等）
  - 对象存储（MinIO）
  - 元数据库（任务、血缘、日志）

---

## 3. 功能需求说明

## 3.1 数据源引接系统

### 3.1.1 多源数据接入
- 支持数据源类型：
  - 关系型数据库：MySQL、PostgreSQL
  - 分析型数据库：ClickHouse
  - 对象存储：MinIO（S3 协议）
  - 文件类：FTP / SFTP / 本地文件

### 3.1.2 数据源配置
- 可视化配置页面：
  - 数据源类型选择
  - 连接参数配置（地址、端口、认证信息）
  - 连接测试与校验
- 支持数据源的新增、编辑、删除

### 3.1.3 数据同步配置
- 同步方式：
  - 全量同步
  - 增量同步（基于时间戳 / 主键）
- 同步对象选择：
  - 表级、字段级
  - 文件路径级
- 同步策略：
  - 定时同步
  - 手动触发

### 3.1.4 同步任务监控
- 任务进度实时展示（Flink / Spark Job）
- 同步状态：
  - 成功 / 失败 / 运行中 / 中断
- 失败原因与日志查看

---

## 3.2 数据管理系统

### 3.2.1 数据资产展示
- 数据资产总览大屏：
  - 已引接数据集列表
  - 数据来源、更新时间、数据量
- 数据分库 / 分表 / 分目录展示

### 3.2.2 数据操作管理
- 针对不同数据类型支持：
  - 增 / 删 / 改 / 查（CRUD）
- 表结构、字段信息可视化
- 数据记录级预览与编辑

### 3.2.3 数据顺序与结构调整
- 支持字段顺序调整
- 支持数据排序规则配置
- 支持表结构变更（字段重命名、类型变更）

### 3.2.4 数据导出与共享
- 导出方式：
  - CSV / Parquet / JSON
- 自动生成下载链接
- 支持链接有效期控制和访问权限校验

---

## 3.3 数据预处理系统

### 3.3.1 预处理任务配置
- 预处理流程可视化编排
- 支持多步骤串联
- 任务可保存为模板

### 3.3.2 时序数据预处理
- 数据清洗：
  - 重复值去除
  - 无效数据过滤
- 结构化整理：
  - 时间对齐
  - 频率重采样
- 异常值处理：
  - 阈值法
  - 统计学方法（3σ）
- 缺失值补全：
  - 插值法
  - 前向 / 后向填充

### 3.3.3 其他数据类型预处理
- 表格数据：
  - 字段标准化
  - 编码转换
- 文件数据：
  - 格式转换
  - 内容抽取

### 3.3.4 结果回写
- 预处理结果可：
  - 覆盖原数据
  - 生成新数据集
- 支持写回原数据库或新库

---

## 3.4 用户行为审查系统

### 3.4.1 操作日志采集
- 记录内容：
  - 用户身份
  - 操作类型
  - 操作对象
  - 时间与结果

### 3.4.2 行为分析与审计
- 支持按用户 / 时间 / 操作类型查询
- 支持导出审计日志

### 3.4.3 异常行为预警
- 异常规则示例：
  - 高频删除操作
  - 非授权数据访问
- 支持告警方式：
  - 系统通知
  - 邮件 / 消息推送

---

## 4. 非功能性需求

- 高并发任务调度能力
- 数据同步与处理的容错与重试机制
- 数据传输与存储安全（加密、鉴权）
- 系统可扩展性与插件化设计

---

## 5. 技术约束

- 后端计算引擎：Flink、Spark
- 支持批处理与流处理混合模式
- 前后端分离架构

---

## 6. 系统功能点汇总表

| 模块 | 子模块 | 功能点 | 功能说明 |
|---|---|---|---|
| 数据源引接 | 多源接入 | 数据库接入 | 支持 MySQL / ClickHouse |
| 数据源引接 | 配置管理 | 数据源配置 | 可视化配置与测试 |
| 数据源引接 | 同步管理 | 同步策略 | 全量 / 增量同步 |
| 数据源引接 | 任务监控 | 状态展示 | 进度与日志查看 |
| 数据管理 | 数据展示 | 数据资产总览 | 已引接数据可视化 |
| 数据管理 | 数据操作 | CRUD | 表级、记录级操作 |
| 数据管理 | 结构调整 | 顺序与结构 | 字段顺序与类型调整 |
| 数据管理 | 数据导出 | 链接下载 | 生成可访问链接 |
| 数据预处理 | 任务配置 | 流程编排 | 可视化预处理流程 |
| 数据预处理 | 时序处理 | 清洗补全 | 异常与缺失处理 |
| 数据预处理 | 结果管理 | 回写存储 | 写回或新建数据集 |
| 行为审查 | 日志采集 | 操作记录 | 全量操作日志 |
| 行为审查 | 行为分析 | 审计查询 | 条件化查询 |
| 行为审查 | 异常预警 | 告警机制 | 规则触发预警 |

---

## 7. 里程碑建议
- M1：数据源引接 + 任务监控
- M2：数据管理与导出
- M3：数据预处理能力
- M4：行为审计与预警

# 数据预处理系统技术实现框架（仅使用 Spark）

> 本文档基于《数据预处理系统 PRD》，从**技术实现视角**重新设计后台架构与处理流程，目标是：
> **让后端工程师、数据工程师能够直接按本文档实现系统**。

---

## 1. 技术选型与总体原则

### 1.1 技术选型（统一 Spark）

| 层级      | 技术                         | 说明                      |
| ------- | -------------------------- | ----------------------- |
| 计算引擎    | Apache Spark               | 统一批处理 + 准实时处理           |
| 流处理能力   | Spark Structured Streaming | 替代 Flink，采用 Micro-Batch |
| SQL 引擎  | Spark SQL                  | 数据管理、预处理                |
| ML / 统计 | Spark MLlib                | 异常检测等                   |
| 存储      | MySQL / ClickHouse / MinIO | 业务数据与对象存储               |
| 元数据     | MySQL                      | 数据源、任务、日志               |

### 1.2 核心设计原则

1. **所有重计算全部通过 Spark Job 完成**
2. **前端不直接操作数据，只生成任务配置**
3. **任务 = 配置 + Spark 模板代码**
4. **禁止原地修改大数据表，统一“读 → 处理 → 写新版本”**

---

## 2. 后台总体技术架构

```
前端（Web UI）
   ↓ REST API
任务配置服务（Java / Python）
   ↓ 生成 Job 参数
Spark Job 提交器（spark-submit）
   ↓
Spark Batch / Structured Streaming
   ↓
目标数据库 / MinIO
```

### 后端服务职责说明

| 服务      | 职责                   |
| ------- | -------------------- |
| 数据源管理服务 | 管理数据源元信息，不读写真实数据     |
| 任务管理服务  | 保存任务配置、触发 Spark Job  |
| 状态监控服务  | 采集 Spark Listener 指标 |
| 行为审计服务  | 记录用户操作日志             |

---

## 3. 数据源引接系统 —— 技术实现

### 3.1 数据源元数据模型（必须实现）

```sql
DataSource(
  id,
  type,              -- mysql / clickhouse / minio / ftp
  connection_json,   -- host、port、bucket 等
  auth_json,
  create_time
)
```

Spark **不保存连接信息**，只在 Job 启动时注入。

---

### 3.2 全量数据引接（Spark Batch Job）

#### 适用场景

* 首次引接
* 定期全量同步

#### 技术流程

```
SparkFullSyncJob
  1. 读取数据源（JDBC / S3A / FTP）
  2. Schema 推断或映射
  3. 数据校验（空值、类型）
  4. 写入目标存储
```

#### 关键实现点

* JDBC 并行读取（partitionColumn）
* MinIO 使用 s3a://
* 写入时统一使用 Parquet / ORC

---

### 3.3 增量 / 准实时引接（Spark Structured Streaming）

#### 适用场景

* 按时间戳增量同步
* 准实时更新

#### 技术流程

```
SparkIncrementalSyncJob
  Source（JDBC / 文件流）
    → Filter（update_time > last_offset）
      → 去重（dropDuplicates）
        → Sink
```

#### 状态管理

* Checkpoint 保存 offset
* offset 也同步写入 MySQL 任务表

> ⚠ 不追求严格 Exactly-Once，通过去重实现幂等

---

## 4. 数据管理系统 —— 技术实现

### 4.1 数据查询

| 场景   | 实现方式          |
| ---- | ------------- |
| 小数据量 | 直连数据库 SQL     |
| 大数据量 | Spark SQL Job |

---

### 4.2 数据增删改（批量）

> 所有修改类操作 **必须走 Spark**

#### 技术流程

```
SparkRewriteJob
  Read 原表
    → Apply 更新/删除规则
      → 写入新表版本
```

#### 示例

* 删除条件：where status = 'invalid'
* 更新：生成新字段或覆盖字段

---

### 4.3 表结构 / 字段顺序调整

#### 技术流程

```
SparkSchemaMigrationJob
  Read 表
    → select(colA, colC, colB)
      → cast(colB as int)
        → Write new_table_v2
```

前端仅配置“字段顺序 / 类型”，Spark 执行。

---

### 4.4 数据导出

```
SparkExportJob
  Read Dataset
    → Format 转换（CSV/JSON）
      → Write MinIO
```

导出完成后生成：

* 文件路径
* 下载 token
* 过期时间

---

## 5. 数据预处理系统 —— Spark 核心实现

### 5.1 预处理任务通用模型

```json
{
  "task_id": "xxx",
  "input_dataset": "table_v1",
  "operators": [
    {"type": "dedup"},
    {"type": "missing_fill", "method": "linear"},
    {"type": "outlier", "method": "zscore"}
  ],
  "output": "table_v2"
}
```

后端根据该配置 **动态拼接 Spark 算子**。

---

### 5.2 时序数据预处理（Spark）

#### 算子执行顺序

```
Read TimeSeries
  → 时间对齐
    → 缺失值补全
      → 异常值处理
        → Write
```

#### 技术要点

* Window + orderBy
* Watermark（准实时）
* UDF / Spark SQL

---

### 5.3 异常值处理实现方式

| 方法 | Spark 实现                  |
| -- | ------------------------- |
| 阈值 | where col > max           |
| 3σ | avg + stddev              |
| ML | Spark ML Isolation Forest |

---

### 5.4 结果回写策略

| 模式 | 实现                 |
| -- | ------------------ |
| 覆盖 | truncate + insert  |
| 新表 | table_v2           |
| 快照 | append snapshot_id |

---

## 6. 用户行为审计系统 —— Spark Streaming

### 6.1 行为日志采集

```text
UserActionLog
- user_id
- action
- resource
- timestamp
```

由业务系统直接写入日志表 / Kafka。

---

### 6.2 异常行为检测

```
SparkUserAuditJob
  Read Action Stream
    → Window 聚合
      → 规则判断
        → 写入告警表
```

示例规则：

* 5 分钟内删除 ≥ N 次
* 非授权导出

---

## 7. Spark Job 代码组织建议

```
/jobs
  ├── full_sync_job.py
  ├── incremental_sync_job.py
  ├── preprocess_job.py
  ├── export_job.py
  └── user_audit_job.py

/operators
  ├── dedup.py
  ├── missing_fill.py
  ├── outlier.py
  └── normalize.py
```

